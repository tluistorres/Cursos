{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5AyYFOKSNEWheFa8M7Dtz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tluistorres/Cursos/blob/main/Challenge3_Data_Science.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  # Challenge3_Data_Science"
      ],
      "metadata": {
        "id": "UmbBPq1lvaQM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chut_de_clientes"
      ],
      "metadata": {
        "id": "HMv5no9jvxd2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_S2PHh4u8zZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('/content/TelecomX_Data.json', 'r') as arquivo:\n",
        "    dados = json.load(arquivo)\n",
        "    df = pd.DataFrame(dados)\n",
        "\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUqFDbBrycKS",
        "outputId": "caf1b2bd-208c-478c-c437-df65950e845a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   customerID Churn                                           customer  \\\n",
            "0  0002-ORFBO    No  {'gender': 'Female', 'SeniorCitizen': 0, 'Part...   \n",
            "1  0003-MKNFE    No  {'gender': 'Male', 'SeniorCitizen': 0, 'Partne...   \n",
            "2  0004-TLHLJ   Yes  {'gender': 'Male', 'SeniorCitizen': 0, 'Partne...   \n",
            "3  0011-IGKFF   Yes  {'gender': 'Male', 'SeniorCitizen': 1, 'Partne...   \n",
            "4  0013-EXCHZ   Yes  {'gender': 'Female', 'SeniorCitizen': 1, 'Part...   \n",
            "\n",
            "                                             phone  \\\n",
            "0   {'PhoneService': 'Yes', 'MultipleLines': 'No'}   \n",
            "1  {'PhoneService': 'Yes', 'MultipleLines': 'Yes'}   \n",
            "2   {'PhoneService': 'Yes', 'MultipleLines': 'No'}   \n",
            "3   {'PhoneService': 'Yes', 'MultipleLines': 'No'}   \n",
            "4   {'PhoneService': 'Yes', 'MultipleLines': 'No'}   \n",
            "\n",
            "                                            internet  \\\n",
            "0  {'InternetService': 'DSL', 'OnlineSecurity': '...   \n",
            "1  {'InternetService': 'DSL', 'OnlineSecurity': '...   \n",
            "2  {'InternetService': 'Fiber optic', 'OnlineSecu...   \n",
            "3  {'InternetService': 'Fiber optic', 'OnlineSecu...   \n",
            "4  {'InternetService': 'Fiber optic', 'OnlineSecu...   \n",
            "\n",
            "                                             account  \n",
            "0  {'Contract': 'One year', 'PaperlessBilling': '...  \n",
            "1  {'Contract': 'Month-to-month', 'PaperlessBilli...  \n",
            "2  {'Contract': 'Month-to-month', 'PaperlessBilli...  \n",
            "3  {'Contract': 'Month-to-month', 'PaperlessBilli...  \n",
            "4  {'Contract': 'Month-to-month', 'PaperlessBilli...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Supondo que o dataframe seja chamado de 'df'\n",
        "df_customer = pd.json_normalize(df['customer'])\n",
        "df_phone = pd.json_normalize(df['phone'])\n",
        "df_internet = pd.json_normalize(df['internet'])\n",
        "df_account = pd.json_normalize(df['account'])\n",
        "\n",
        "# Concatenar os dataframes\n",
        "df_final = pd.concat([df[['customerID', 'Churn']], df_customer, df_phone, df_internet, df_account], axis=1)\n",
        "\n",
        "print(df_final.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7DZsjWB0stv",
        "outputId": "713b425e-4748-4a4a-d14c-b7065316adae"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   customerID Churn  gender  SeniorCitizen Partner Dependents  tenure  \\\n",
            "0  0002-ORFBO    No  Female              0     Yes        Yes       9   \n",
            "1  0003-MKNFE    No    Male              0      No         No       9   \n",
            "2  0004-TLHLJ   Yes    Male              0      No         No       4   \n",
            "3  0011-IGKFF   Yes    Male              1     Yes         No      13   \n",
            "4  0013-EXCHZ   Yes  Female              1     Yes         No       3   \n",
            "\n",
            "  PhoneService MultipleLines InternetService  ... OnlineBackup  \\\n",
            "0          Yes            No             DSL  ...          Yes   \n",
            "1          Yes           Yes             DSL  ...           No   \n",
            "2          Yes            No     Fiber optic  ...           No   \n",
            "3          Yes            No     Fiber optic  ...          Yes   \n",
            "4          Yes            No     Fiber optic  ...           No   \n",
            "\n",
            "  DeviceProtection TechSupport StreamingTV StreamingMovies        Contract  \\\n",
            "0               No         Yes         Yes              No        One year   \n",
            "1               No          No          No             Yes  Month-to-month   \n",
            "2              Yes          No          No              No  Month-to-month   \n",
            "3              Yes          No         Yes             Yes  Month-to-month   \n",
            "4               No         Yes         Yes              No  Month-to-month   \n",
            "\n",
            "  PaperlessBilling     PaymentMethod Charges.Monthly  Charges.Total  \n",
            "0              Yes      Mailed check            65.6          593.3  \n",
            "1               No      Mailed check            59.9          542.4  \n",
            "2              Yes  Electronic check            73.9         280.85  \n",
            "3              Yes  Electronic check            98.0        1237.85  \n",
            "4              Yes      Mailed check            83.9          267.4  \n",
            "\n",
            "[5 rows x 21 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora que os dados estão normalizados, você pode começar a explorar e analisar os dados.\n",
        "\n",
        "Algumas sugestões de análises que você pode fazer:\n",
        "\n",
        "1. Análise de churn: Qual é a proporção de clientes que abandonaram o serviço (Churn = Yes)? Quais são as características desses clientes?\n",
        "2. Análise de gênero: Existem diferenças significativas entre homens e mulheres em termos de churn ou uso de serviços?\n",
        "3. Análise de serviços: Quais são os serviços mais populares (InternetService, PhoneService, etc.)? Existem relações entre os serviços e o churn?\n",
        "4. Análise de contrato: Qual é o tipo de contrato mais comum (Month-to-month, One year, etc.)? Existem relações entre o tipo de contrato e o churn?\n",
        "5. Análise de pagamento: Qual é o método de pagamento mais comum (Mailed check, Electronic check, etc.)? Existem relações entre o método de pagamento e o churn?\n",
        "\n",
        "Você pode usar técnicas estatísticas e de visualização de dados para explorar esses tópicos. Algumas opções incluem:\n",
        "\n",
        "- Estatísticas descritivas: Calcular médias, medianas, modas, etc. para entender melhor os dados.\n",
        "- Gráficos de barras: Visualizar a distribuição de variáveis categóricas (gênero, tipo de contrato, etc.).\n",
        "- Gráficos de dispersão: Visualizar a relação entre variáveis contínuas (Charges.Monthly, Charges.Total, etc.).\n",
        "- Análise de correlação: Calcular a correlação entre variáveis para entender melhor as relações entre elas.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "4QRcWZc-1fTH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora que você fez a extração e transformação dos dados, a próxima etapa é a carga (ou carregamento) dos dados em um destino final, como um banco de dados, um arquivo CSV, etc.\n",
        "\n",
        "Aqui estão algumas opções para carregar os dados:\n",
        "\n",
        "1. Carregar em um banco de dados: Você pode usar bibliotecas como pandas e sqlalchemy para carregar os dados em um banco de dados relacional, como MySQL, PostgreSQL, etc.\n",
        "\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "# Criar uma conexão com o banco de dados\n",
        "engine = create_engine('postgresql://usuario:senha@host:porta/banco_de_dados')\n",
        "\n",
        "# Carregar os dados no banco de dados\n",
        "df.to_sql('nome_da_tabela', engine, if_exists='replace', index=False)\n",
        "\n",
        "2. Carregar em um arquivo CSV: Você pode usar a função to_csv do pandas para carregar os dados em um arquivo CSV.\n",
        "\n",
        "df.to_csv('nome_do_arquivo.csv', index=False)\n",
        "\n",
        "3. Carregar em um arquivo Excel: Você pode usar a função to_excel do pandas para carregar os dados em um arquivo Excel.\n",
        "\n",
        "df.to_excel('nome_do_arquivo.xlsx', index=False)\n",
        "\n",
        "4. Carregar em um sistema de armazenamento de dados: Você pode usar bibliotecas como pandas e boto3 para carregar os dados em um sistema de armazenamento de dados, como Amazon S3, Google Cloud Storage, etc.\n",
        "\n",
        "A escolha do destino final depende das necessidades específicas do seu projeto. Se você tiver alguma dúvida ou precisar de ajuda com a carga dos dados, estou aqui para ajudar!\n",
        "\n",
        "Lembre-se de que a carga dos dados pode envolver considerações adicionais, como:\n",
        "\n",
        "- Tratamento de erros: Como lidar com erros que possam ocorrer durante a carga dos dados?\n",
        "- Validação de dados: Como garantir que os dados sejam válidos e consistentes antes de carregá-los?\n",
        "- Segurança: Como garantir que os dados sejam carregados de forma segura e protegida?"
      ],
      "metadata": {
        "id": "mnEACSqQ2ObN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('TeleconX_Data.csv', index=False, header=True, sep=';', encoding='utf-8')"
      ],
      "metadata": {
        "id": "jvd91e5h3AYb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('TeleconX_Data.csv', sep=';')\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcvH_0sz3ncr",
        "outputId": "af608464-9135-47e0-d6c9-fd405d98e389"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   customerID Churn                                           customer  \\\n",
            "0  0002-ORFBO    No  {'gender': 'Female', 'SeniorCitizen': 0, 'Part...   \n",
            "1  0003-MKNFE    No  {'gender': 'Male', 'SeniorCitizen': 0, 'Partne...   \n",
            "2  0004-TLHLJ   Yes  {'gender': 'Male', 'SeniorCitizen': 0, 'Partne...   \n",
            "3  0011-IGKFF   Yes  {'gender': 'Male', 'SeniorCitizen': 1, 'Partne...   \n",
            "4  0013-EXCHZ   Yes  {'gender': 'Female', 'SeniorCitizen': 1, 'Part...   \n",
            "\n",
            "                                             phone  \\\n",
            "0   {'PhoneService': 'Yes', 'MultipleLines': 'No'}   \n",
            "1  {'PhoneService': 'Yes', 'MultipleLines': 'Yes'}   \n",
            "2   {'PhoneService': 'Yes', 'MultipleLines': 'No'}   \n",
            "3   {'PhoneService': 'Yes', 'MultipleLines': 'No'}   \n",
            "4   {'PhoneService': 'Yes', 'MultipleLines': 'No'}   \n",
            "\n",
            "                                            internet  \\\n",
            "0  {'InternetService': 'DSL', 'OnlineSecurity': '...   \n",
            "1  {'InternetService': 'DSL', 'OnlineSecurity': '...   \n",
            "2  {'InternetService': 'Fiber optic', 'OnlineSecu...   \n",
            "3  {'InternetService': 'Fiber optic', 'OnlineSecu...   \n",
            "4  {'InternetService': 'Fiber optic', 'OnlineSecu...   \n",
            "\n",
            "                                             account  \n",
            "0  {'Contract': 'One year', 'PaperlessBilling': '...  \n",
            "1  {'Contract': 'Month-to-month', 'PaperlessBilli...  \n",
            "2  {'Contract': 'Month-to-month', 'PaperlessBilli...  \n",
            "3  {'Contract': 'Month-to-month', 'PaperlessBilli...  \n",
            "4  {'Contract': 'Month-to-month', 'PaperlessBilli...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para entender os fatores que levam à perda de clientes, podemos seguir os seguintes passos:\n",
        "\n",
        "1. Coletar os dados: Coletar os dados de clientes, incluindo informações demográficas, comportamentais e de uso do serviço.\n",
        "2. Tratar os dados: Limpar e preparar os dados para análise, lidando com valores ausentes, erros de digitação e outros problemas.\n",
        "3. Analisar os dados: Utilizar técnicas estatísticas e de aprendizado de máquina para identificar padrões e relações nos dados.\n",
        "\n",
        "Aqui está um exemplo de como podemos fazer isso utilizando Python e suas bibliotecas:"
      ],
      "metadata": {
        "id": "ogLXwYV75Hb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Carregar os dados\n",
        "\n",
        "df = pd.read_csv('TeleconX_Data.csv', sep=';')\n",
        "\n"
      ],
      "metadata": {
        "id": "bDV6QfLq5MH5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tratar os dados\n",
        "df = df.dropna()  # remover linhas com valores ausentes\n",
        "df = df.drop_duplicates()  # remover linhas duplicadas\n",
        "\n"
      ],
      "metadata": {
        "id": "LI3W6s_S5pMw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUVn6EeX522R",
        "outputId": "eff1c28d-f5bd-48d0-b590-d66198ad8b31"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   customerID Churn                                           customer  \\\n",
            "0  0002-ORFBO    No  {'gender': 'Female', 'SeniorCitizen': 0, 'Part...   \n",
            "1  0003-MKNFE    No  {'gender': 'Male', 'SeniorCitizen': 0, 'Partne...   \n",
            "2  0004-TLHLJ   Yes  {'gender': 'Male', 'SeniorCitizen': 0, 'Partne...   \n",
            "3  0011-IGKFF   Yes  {'gender': 'Male', 'SeniorCitizen': 1, 'Partne...   \n",
            "4  0013-EXCHZ   Yes  {'gender': 'Female', 'SeniorCitizen': 1, 'Part...   \n",
            "\n",
            "                                             phone  \\\n",
            "0   {'PhoneService': 'Yes', 'MultipleLines': 'No'}   \n",
            "1  {'PhoneService': 'Yes', 'MultipleLines': 'Yes'}   \n",
            "2   {'PhoneService': 'Yes', 'MultipleLines': 'No'}   \n",
            "3   {'PhoneService': 'Yes', 'MultipleLines': 'No'}   \n",
            "4   {'PhoneService': 'Yes', 'MultipleLines': 'No'}   \n",
            "\n",
            "                                            internet  \\\n",
            "0  {'InternetService': 'DSL', 'OnlineSecurity': '...   \n",
            "1  {'InternetService': 'DSL', 'OnlineSecurity': '...   \n",
            "2  {'InternetService': 'Fiber optic', 'OnlineSecu...   \n",
            "3  {'InternetService': 'Fiber optic', 'OnlineSecu...   \n",
            "4  {'InternetService': 'Fiber optic', 'OnlineSecu...   \n",
            "\n",
            "                                             account  \n",
            "0  {'Contract': 'One year', 'PaperlessBilling': '...  \n",
            "1  {'Contract': 'Month-to-month', 'PaperlessBilli...  \n",
            "2  {'Contract': 'Month-to-month', 'PaperlessBilli...  \n",
            "3  {'Contract': 'Month-to-month', 'PaperlessBilli...  \n",
            "4  {'Contract': 'Month-to-month', 'PaperlessBilli...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bPhFYZ-KvXnm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converter variáveis categóricas em numéricas\n",
        "\n",
        "df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})\n",
        "\n"
      ],
      "metadata": {
        "id": "kYv5FFbF6FG7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzH-UcKi6mTR",
        "outputId": "c089ef2d-45ab-4471-bbbe-650370ce500b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['customerID', 'Churn', 'customer', 'phone', 'internet', 'account'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df.drop('Churn', axis=1)\n",
        "y = df['Churn']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "jmHN3zhw7ckG"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para dividir o dataframe em treino e teste com 70% para treino e 30% para teste, você pode utilizar a função train_test_split da biblioteca scikit-learP"
      ],
      "metadata": {
        "id": "L1d8YUy18KWO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nesse exemplo, test_size=0.3 significa que 30% dos dados serão utilizados para teste, e o restante (70%) será utilizado para treino.\n",
        "\n",
        "Se você quiser especificar a proporção de treino explicitamente, você pode utilizar train_size=0.7:\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=42)\n",
        "\n",
        "Ambas as abordagens produzirão o mesmo resultado, com 70% dos dados sendo utilizados para treino e 30% para teste.\n",
        "\n",
        "O parâmetro random_state=42 é utilizado para garantir que a divisão seja reproduzível, ou seja, que você obtenha os mesmos resultados se executar o código novamente. Se você quiser uma divisão aleatória, pode omitir esse parâmetro ou utilizar um valor diferente."
      ],
      "metadata": {
        "id": "oQ4J0CoJ8Rh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Remover a coluna customerID:\n",
        "\n",
        "X_train = X_train.drop('customerID', axis=1)\n",
        "X_test = X_test.drop('customerID', axis=1)\n"
      ],
      "metadata": {
        "id": "xbyJzQjn88L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.drop('customerID', axis=1)\n",
        "X_test = X_test.drop('customerID', axis=1)\n"
      ],
      "metadata": {
        "id": "ErqJueVN8-pp"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parece que as colunas X_train ainda contêm valores categóricos não numéricos, como 'Male', 'Yes', 'No', etc.\n",
        "\n",
        "Para resolver isso, você precisa pré-processar os dados para transformar as colunas categóricas em numéricas. Aqui estão algumas opções:\n",
        "\n",
        "1. Utilizar LabelEncoder para transformar as colunas categóricas em numéricas:\n"
      ],
      "metadata": {
        "id": "t8Islz3C9okd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Supondo que X_train e X_test sejam dataframes com uma coluna 'Charges' que contém um dicionário\n",
        "X_train = pd.json_normalize(X_train, sep='_')\n",
        "X_test = pd.json_normalize(X_test, sep='_')"
      ],
      "metadata": {
        "id": "OENkyC6W8ZDD"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parece que o problema está ocorrendo porque as colunas X_train e X_test têm estruturas diferentes, e o LabelEncoder está sendo treinado em uma estrutura de dados que não é compatível com a estrutura de dados que está sendo transformada.\n",
        "\n",
        "Além disso, parece que as colunas Charges contém um dicionário com valores numéricos e string, o que não é compatível com o LabelEncoder.\n",
        "\n",
        "Para resolver isso, você precisa garantir que as colunas X_train e X_test tenham a mesma estrutura e que os valores sejam compatíveis com o LabelEncoder.\n",
        "\n",
        "Aqui estão algumas sugestões:\n",
        "\n",
        "1. Verifique se as colunas X_train e X_test têm a mesma estrutura e conteúdo.\n",
        "2. Se as colunas Charges contêm um dicionário, você pode precisar extrair os valores numéricos e string separadamente e processá-los de forma diferente.\n",
        "3. Utilize pd.json_normalize para normalizar os dados JSON e criar colunas separadas para cada valor.\n"
      ],
      "metadata": {
        "id": "xhXwLJmq-YBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Supondo que X_train e X_test sejam dataframes com uma coluna 'Charges' que contém um dicionário\n",
        "\n",
        "X_train = pd.json_normalize(X_train, sep='_')\n",
        "X_test = pd.json_normalize(X_test, sep='_')"
      ],
      "metadata": {
        "id": "OnsV8qmp-d-V"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Você pode utilizar o seguinte código para identificar as colunas categóricas e numéricas:\n",
        "\n",
        "categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
        "numerical_cols = X_train.select_dtypes(exclude=['object']).columns\n",
        "\n",
        "Em seguida, você pode utilizar o LabelEncoder para transformar as colunas categóricas em numéricas:\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder( )\n",
        "for col in categorical_cols:\n",
        "    X_train[col] = le.fit_transform(X_train[col])\n",
        "    X_test[col] = le.transform(X_test[col])"
      ],
      "metadata": {
        "id": "l9YF3cy0-19t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
        "numerical_cols = X_train.select_dtypes(exclude=['object']).columns"
      ],
      "metadata": {
        "id": "QsABMaYD-4Ow"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "for col in categorical_cols:\n",
        "    X_train[col] = le.fit_transform(X_train[col])\n",
        "    X_test[col] = le.transform(X_test[col])\n"
      ],
      "metadata": {
        "id": "A9AC4fGM-__O"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print('Acurácia:', accuracy_score(y_test, y_pred))\n",
        "print('Relatório de classificação:')\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "YuyXpvs7_V37",
        "outputId": "9aa8b9f8-6fc4-4e04-d89b-9425f6b344ee"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "at least one array or dtype is required",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-477055565>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Acurácia:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m         X, y = validate_data(\n\u001b[0m\u001b[1;32m    361\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2959\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2960\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2961\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2962\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0mensure_all_finite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deprecate_force_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_all_finite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m   1371\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    929\u001b[0m         )\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdtype_iter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdtypes_orig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m             \u001b[0mdtype_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdtypes_orig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpandas_requires_conversion\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdtypes_orig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;31m# Force object if any of the dtypes is an object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: at least one array or dtype is required"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parece que o problema está ocorrendo porque os dados X_train ou y_train estão vazios ou não têm o formato correto.\n",
        "\n",
        "Aqui estão algumas coisas que você pode verificar:\n",
        "\n",
        "1. Verifique se os dados X_train e y_train não estão vazios:\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n"
      ],
      "metadata": {
        "id": "y-yVS8-__rNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(X_train))\n",
        "print(type(y_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTcmKyQB_sXV",
        "outputId": "6dc70273-7830-4caf-8576-e87bd5fe9b8c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'pandas.core.series.Series'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.isnull().sum())\n",
        "print(y_train.isnull().sum())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g15EWD3M_7wk",
        "outputId": "ae79fdcc-8fa6-4812-fbd9-f15087540222"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Series([], dtype: float64)\n",
            "4930\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_m--kqyAEhh",
        "outputId": "c7faba7e-86a9-4f1c-a01d-6116b62b0a04"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Series([], dtype: object)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Com base na saída, parece que y_train tem 4930 linhas e não há valores nulos. Já X_train parece estar vazio, pois a saída Series([], dtype: float64) indica que não há colunas com valores.\n",
        "\n",
        "Isso pode ser o motivo pelo qual o modelo não está funcionando. Você precisa verificar por que X_train está vazio e garantir que ele contenha os dados necessários para treinar o modelo.\n",
        "\n",
        "Aqui estão algumas coisas que você pode verificar:\n",
        "\n",
        "1. Verifique se os dados foram carregados corretamente e se as variáveis X_train e y_train foram criadas corretamente.\n",
        "2. Verifique se houve algum erro durante o pré-processamento dos dados que possa ter causado a perda de dados.\n",
        "3. Verifique se as colunas de X foram selecionadas corretamente antes de dividir os dados em treino e teste.\n",
        "\n",
        "Se você tiver alguma dúvida ou precisar de mais ajuda, estou aqui para ajudar!\n",
        "\n",
        "Você pode tentar imprimir X_train para ver se ele está realmente vazio:\n",
        "\n",
        "print(X_train)\n",
        "\n",
        "Se X_train estiver vazio, você precisará encontrar uma maneira de preencher ele com os dados necessários."
      ],
      "metadata": {
        "id": "yAxfqH7XAa7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkIKCX1lAcNY",
        "outputId": "a35794e1-a28e-4d74-b174-44111ae70965"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sim, X_train está vazio. Isso significa que não há dados para treinar o modelo.\n",
        "\n",
        "Você precisa verificar por que X_train está vazio e garantir que os dados sejam carregados corretamente e divididos em treino e teste.\n",
        "\n",
        "Aqui estão algumas coisas que você pode verificar:\n",
        "\n",
        "1. Verifique se os dados foram carregados corretamente:\n",
        "\n",
        "print(df.head())\n",
        "\n",
        "2. Verifique se as variáveis X e y foram criadas corretamente:\n",
        "\n",
        "X = df.drop('Churn', axis=1)\n",
        "y = df['Churn']\n",
        "print(X.head())\n",
        "print(y.head())\n",
        "\n",
        "3. Verifique se a divisão dos dados em treino e teste foi feita corretamente:\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "\n",
        "Se você encontrar o problema, você pode corrigi-lo e tentar novamente. Se você tiver alguma dúvida ou precisar de mais ajuda, estou aqui para ajudar!"
      ],
      "metadata": {
        "id": "YC_K3BwkAv7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTZAr9MJAxQ2",
        "outputId": "c9d89bfa-a476-4e84-e320-fe7077a2d778"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   customerID  Churn                                           customer  \\\n",
            "0  0002-ORFBO    NaN  {'gender': 'Female', 'SeniorCitizen': 0, 'Part...   \n",
            "1  0003-MKNFE    NaN  {'gender': 'Male', 'SeniorCitizen': 0, 'Partne...   \n",
            "2  0004-TLHLJ    NaN  {'gender': 'Male', 'SeniorCitizen': 0, 'Partne...   \n",
            "3  0011-IGKFF    NaN  {'gender': 'Male', 'SeniorCitizen': 1, 'Partne...   \n",
            "4  0013-EXCHZ    NaN  {'gender': 'Female', 'SeniorCitizen': 1, 'Part...   \n",
            "\n",
            "                                             phone  \\\n",
            "0   {'PhoneService': 'Yes', 'MultipleLines': 'No'}   \n",
            "1  {'PhoneService': 'Yes', 'MultipleLines': 'Yes'}   \n",
            "2   {'PhoneService': 'Yes', 'MultipleLines': 'No'}   \n",
            "3   {'PhoneService': 'Yes', 'MultipleLines': 'No'}   \n",
            "4   {'PhoneService': 'Yes', 'MultipleLines': 'No'}   \n",
            "\n",
            "                                            internet  \\\n",
            "0  {'InternetService': 'DSL', 'OnlineSecurity': '...   \n",
            "1  {'InternetService': 'DSL', 'OnlineSecurity': '...   \n",
            "2  {'InternetService': 'Fiber optic', 'OnlineSecu...   \n",
            "3  {'InternetService': 'Fiber optic', 'OnlineSecu...   \n",
            "4  {'InternetService': 'Fiber optic', 'OnlineSecu...   \n",
            "\n",
            "                                             account  \n",
            "0  {'Contract': 'One year', 'PaperlessBilling': '...  \n",
            "1  {'Contract': 'Month-to-month', 'PaperlessBilli...  \n",
            "2  {'Contract': 'Month-to-month', 'PaperlessBilli...  \n",
            "3  {'Contract': 'Month-to-month', 'PaperlessBilli...  \n",
            "4  {'Contract': 'Month-to-month', 'PaperlessBilli...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parece que os dados estão no formato de dicionários dentro de colunas, o que pode estar causando problemas ao tentar dividir os dados em treino e teste.\n",
        "\n",
        "Você pode tentar utilizar a função pd.json_normalize para normalizar os dados e criar colunas separadas para cada chave do dicionário.\n",
        "\n",
        "Aqui está um exemplo de como você pode fazer isso:\n",
        "\n",
        "df = pd.json_normalize(df, sep='_')\n",
        "\n",
        "Isso irá criar novas colunas para cada chave do dicionário, como customer_gender, phone_PhoneService, internet_InternetService, etc.\n",
        "\n",
        "Depois de normalizar os dados, você pode tentar dividir os dados em treino e teste novamente:\n",
        "\n",
        "X = df.drop(['Churn', 'customerID'], axis=1)\n",
        "y = df['Churn']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
      ],
      "metadata": {
        "id": "GH1hjySzA_z3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.json_normalize(df, sep='_')"
      ],
      "metadata": {
        "id": "LZm06B7dBA3v"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(['Churn', 'customerID'], axis=1)\n",
        "y = df['Churn']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "LsVhFiNSBKqy",
        "outputId": "a99906fd-c5e9-4469-9e6b-8ae8fe37d441"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['Churn', 'customerID'] not found in axis\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-978368699>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Churn'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'customerID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Churn'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5579\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[0;36m1.0\u001b[0m     \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5580\u001b[0m         \"\"\"\n\u001b[0;32m-> 5581\u001b[0;31m         return super().drop(\n\u001b[0m\u001b[1;32m   5582\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5583\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4786\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4787\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4788\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4790\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4828\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4829\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4830\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4831\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   7068\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7069\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7070\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask].tolist()} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7071\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7072\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['Churn', 'customerID'] not found in axis\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O erro KeyError indica que as colunas Churn e customerID não existem no dataframe df após a normalização.\n",
        "\n",
        "Isso é provável porque a normalização dos dados pode ter alterado a estrutura das colunas.\n",
        "\n",
        "Você pode verificar as colunas do dataframe df após a normalização:\n",
        "\n",
        "print(df.columns)\n",
        "\n",
        "Isso irá mostrar as colunas que existem no dataframe df após a normalização.\n",
        "\n",
        "Você pode então ajustar o código para selecionar as colunas corretas para X e y.\n",
        "\n",
        "Por exemplo, se a coluna Churn não existir mais, você pode precisar selecionar uma coluna diferente como alvo."
      ],
      "metadata": {
        "id": "sd02700ZBZ_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mupwyVQBbJV",
        "outputId": "a350a58b-daa1-4898-be73-8483d7da5a66"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index([], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parece que o dataframe df está vazio após a normalização.\n",
        "\n",
        "Isso pode ter ocorrido porque a função pd.json_normalize não foi capaz de normalizar os dados corretamente.\n",
        "\n",
        "Você pode tentar normalizar os dados de uma maneira diferente, como normalizar cada coluna que contém dicionários separadamente.\n",
        "\n",
        "Por exemplo:\n",
        "\n",
        "df_customer = pd.json_normalize(df['customer'])\n",
        "df_phone = pd.json_normalize(df['phone'])\n",
        "df_internet = pd.json_normalize(df['internet'])\n",
        "df_account = pd.json_normalize(df['account'])\n",
        "\n",
        "df = pd.concat([df[['customerID', 'Churn']], df_customer, df_phone, df_internet, df_account], axis=1)\n",
        "\n",
        "Isso pode ajudar a normalizar os dados corretamente e criar um dataframe com as colunas certas."
      ],
      "metadata": {
        "id": "v9w0ZIsIBpR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_customer = pd.json_normalize(df['customer'])\n",
        "df_phone = pd.json_normalize(df['phone'])\n",
        "df_internet = pd.json_normalize(df['internet'])\n",
        "df_account = pd.json_normalize(df['account'])\n",
        "\n",
        "df = pd.concat([df[['customerID', 'Churn']], df_customer, df_phone, df_internet, df_account], axis=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "z6v1yM0tBqYg",
        "outputId": "7d1528a5-155f-4fa5-d49e-369218d9086e"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'customer'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'customer'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-3314268645>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_customer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'customer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_phone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'phone'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_internet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'internet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_account\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'account'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'customer'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCthDMUnB8qQ",
        "outputId": "0225f72d-da50-4322-9b05-eb4d1cc4ac4c"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index([], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8njsYiACJwq",
        "outputId": "a9531e7b-696a-4da6-88a6-5fdbac4374b5"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: [0, 1, 2, 3, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns = df.columns.get_level_values(0)"
      ],
      "metadata": {
        "id": "CcBriZT5CjD1"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in df.columns:\n",
        "    if df[col].apply(lambda x: isinstance(x, dict)).any():\n",
        "        temp_df = df[col].apply(pd.Series)\n",
        "        df = pd.concat([df.drop(col, axis=1), temp_df], axis=1)\n"
      ],
      "metadata": {
        "id": "4Wos5ZfLDROo"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yt2xlhWSDcLK",
        "outputId": "0aa144fe-3df7-4bb0-8e88-47d580c375fc"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index([], dtype='object')\n",
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: [0, 1, 2, 3, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({\n",
        "    'customerID': df['customerID'],\n",
        "    'Churn': df['Churn'],\n",
        "    'customer': df['customer'],\n",
        "    'phone': df['phone'],\n",
        "    'internet': df['internet'],\n",
        "    'account': df['account']\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "LgWxzAyvEBek",
        "outputId": "001e05ee-73ec-43f3-8150-d2cdb2f76219"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'customerID'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'customerID'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-371186521>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m df = pd.DataFrame({\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;34m'customerID'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'customerID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m'Churn'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Churn'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m'customer'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'customer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m'phone'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'phone'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'customerID'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = {\n",
        "    'customerID': ['0002-ORFBO', '0003-MKNFE', '0004-TLHLJ', '0011-IGKFF', '0013-EXCHZ'],\n",
        "    'Churn': [None, None, None, None, None],\n",
        "    'customer': [{'gender': 'Female', 'SeniorCitizen': 0}, {'gender': 'Male', 'SeniorCitizen': 0}, {'gender': 'Male', 'SeniorCitizen': 0}, {'gender': 'Male', 'SeniorCitizen': 1}, {'gender': 'Female', 'SeniorCitizen': 1}],\n",
        "    'phone': [{'PhoneService': 'Yes', 'MultipleLines': 'No'}, {'PhoneService': 'Yes', 'MultipleLines': 'Yes'}, {'PhoneService': 'Yes', 'MultipleLines': 'No'}, {'PhoneService': 'Yes', 'MultipleLines': 'No'}, {'PhoneService': 'Yes', 'MultipleLines': 'No'}],\n",
        "    'internet': [{'InternetService': 'DSL', 'OnlineSecurity': 'No'}, {'InternetService': 'DSL', 'OnlineSecurity': 'No'}, {'InternetService': 'Fiber optic', 'OnlineSecurity': 'No'}, {'InternetService': 'Fiber optic', 'OnlineSecurity': 'No'}, {'InternetService': 'Fiber optic', 'OnlineSecurity': 'No'}],\n",
        "    'account': [{'Contract': 'One year', 'PaperlessBilling': 'Yes'}, {'Contract': 'Month-to-month', 'PaperlessBilling': 'Yes'}, {'Contract': 'Month-to-month', 'PaperlessBilling': 'Yes'}, {'Contract': 'Month-to-month', 'PaperlessBilling': 'Yes'}, {'Contract': 'Month-to-month', 'PaperlessBilling': 'Yes'}]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "jdLWhEyMEfEo"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_customer = df['customer'].apply(pd.Series)\n",
        "df_phone = df['phone'].apply(pd.Series)\n",
        "df_internet = df['internet'].apply(pd.Series)\n",
        "df_account = df['account'].apply(pd.Series)\n",
        "\n",
        "df = pd.concat([df[['customerID', 'Churn']], df_customer, df_phone, df_internet, df_account], axis=1)\n"
      ],
      "metadata": {
        "id": "y7sWJKSQElGY"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora que você transformou os dados em um formato mais adequado, você pode começar a fazer análises e modelagem.\n",
        "\n",
        "Aqui estão algumas sugestões de próximos passos:\n",
        "\n",
        "1. Verificar a estrutura dos dados: Verifique se os dados estão no formato correto e se há alguma coluna que precise ser ajustada.\n",
        "2. Tratar os dados missing: Verifique se há alguma coluna com dados missing e decida como lidar com eles (por exemplo, remover as linhas com dados missing ou substituir os valores missing por um valor padrão).\n",
        "3. Fazer análises descritivas: Faça análises descritivas dos dados, como calcular médias, medianas, desvios padrão, etc.\n",
        "4. Fazer visualizações: Crie visualizações dos dados para entender melhor as relações entre as variáveis.\n",
        "5. Preparar os dados para modelagem: Prepare os dados para modelagem, por exemplo, criando variáveis dummy para variáveis categóricas.\n",
        "6. Treinar um modelo: Treine um modelo de machine learning para prever a variável Churn.\n",
        "\n",
        "Alguns exemplos de código que você pode usar:\n",
        "\n",
        "# Verificar a estrutura dos dados\n",
        "print(df.head())\n",
        "print(df.info())\n",
        "\n",
        "# Tratar os dados missing\n",
        "df = df.dropna()  # remover as linhas com dados missing\n",
        "\n",
        "# Fazer análises descritivas\n",
        "print(df.describe())\n",
        "\n",
        "# Fazer visualizações\n",
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(df['SeniorCitizen'], df['Churn'])\n",
        "plt.show()\n",
        "\n",
        "# Preparar os dados para modelagem\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "encoder = OneHotEncoder()\n",
        "df_encoded = pd.DataFrame(encoder.fit_transform(df[['gender']]).toarray())\n",
        "\n",
        "# Treinar um modelo\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "X = df.drop(['Churn', 'customerID'], axis=1)\n",
        "y = df['Churn']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "Essas são apenas algumas sugestões, e você pode adaptá-las às suas necessidades específicas. Se você tiver alguma dúvida ou precisar de mais ajuda, estou aqui para ajudar!"
      ],
      "metadata": {
        "id": "I2ys2jKwE2Gn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar a estrutura dos dados\n",
        "print(df.head())\n",
        "print(df.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsJ9J4Z-E35K",
        "outputId": "c0eef456-baf4-41af-e552-9fb07167c806"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   customerID Churn  gender  SeniorCitizen PhoneService MultipleLines  \\\n",
            "0  0002-ORFBO  None  Female              0          Yes            No   \n",
            "1  0003-MKNFE  None    Male              0          Yes           Yes   \n",
            "2  0004-TLHLJ  None    Male              0          Yes            No   \n",
            "3  0011-IGKFF  None    Male              1          Yes            No   \n",
            "4  0013-EXCHZ  None  Female              1          Yes            No   \n",
            "\n",
            "  InternetService OnlineSecurity        Contract PaperlessBilling  \n",
            "0             DSL             No        One year              Yes  \n",
            "1             DSL             No  Month-to-month              Yes  \n",
            "2     Fiber optic             No  Month-to-month              Yes  \n",
            "3     Fiber optic             No  Month-to-month              Yes  \n",
            "4     Fiber optic             No  Month-to-month              Yes  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5 entries, 0 to 4\n",
            "Data columns (total 10 columns):\n",
            " #   Column            Non-Null Count  Dtype \n",
            "---  ------            --------------  ----- \n",
            " 0   customerID        5 non-null      object\n",
            " 1   Churn             0 non-null      object\n",
            " 2   gender            5 non-null      object\n",
            " 3   SeniorCitizen     5 non-null      int64 \n",
            " 4   PhoneService      5 non-null      object\n",
            " 5   MultipleLines     5 non-null      object\n",
            " 6   InternetService   5 non-null      object\n",
            " 7   OnlineSecurity    5 non-null      object\n",
            " 8   Contract          5 non-null      object\n",
            " 9   PaperlessBilling  5 non-null      object\n",
            "dtypes: int64(1), object(9)\n",
            "memory usage: 532.0+ bytes\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Tratar os dados missing\n",
        "df = df.dropna()  # remover as linhas com dados missing\n"
      ],
      "metadata": {
        "id": "65hSw1a2E-5i"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fazer análises descritivas\n",
        "print(df.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Buj2l3LlFLx4",
        "outputId": "e4236caa-15e5-4538-c813-ba89eb9c5f7c"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       SeniorCitizen\n",
            "count            0.0\n",
            "mean             NaN\n",
            "std              NaN\n",
            "min              NaN\n",
            "25%              NaN\n",
            "50%              NaN\n",
            "75%              NaN\n",
            "max              NaN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMDUsiCLF8Wk",
        "outputId": "28b91756-140f-47cf-f420-9fd85a6c4855"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: [customerID, Churn, gender, SeniorCitizen, PhoneService, MultipleLines, InternetService, OnlineSecurity, Contract, PaperlessBilling]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEO-D3LcGCve",
        "outputId": "6f01b8c5-70a3-464b-fcc2-69e5c0fe2d8a"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "customerID          object\n",
            "Churn               object\n",
            "gender              object\n",
            "SeniorCitizen        int64\n",
            "PhoneService        object\n",
            "MultipleLines       object\n",
            "InternetService     object\n",
            "OnlineSecurity      object\n",
            "Contract            object\n",
            "PaperlessBilling    object\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['gender'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqZobNvwFSQv",
        "outputId": "adcf913f-3942-4986-967d-92246c8d5cff"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Series([], Name: count, dtype: int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.empty)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "docBqqLjGiCO",
        "outputId": "77ae776a-007d-4575-b498-ac7cccaa8ed4"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/TelecomX_Data.json', 'r') as arquivo:\n",
        "    dados = json.load(arquivo)\n",
        "    df = pd.DataFrame(dados)\n",
        "\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_NlfPnHGyE8",
        "outputId": "af55baca-c30a-4e90-80f7-a0b33c7e511c"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   customerID Churn                                           customer  \\\n",
            "0  0002-ORFBO    No  {'gender': 'Female', 'SeniorCitizen': 0, 'Part...   \n",
            "1  0003-MKNFE    No  {'gender': 'Male', 'SeniorCitizen': 0, 'Partne...   \n",
            "2  0004-TLHLJ   Yes  {'gender': 'Male', 'SeniorCitizen': 0, 'Partne...   \n",
            "3  0011-IGKFF   Yes  {'gender': 'Male', 'SeniorCitizen': 1, 'Partne...   \n",
            "4  0013-EXCHZ   Yes  {'gender': 'Female', 'SeniorCitizen': 1, 'Part...   \n",
            "\n",
            "                                             phone  \\\n",
            "0   {'PhoneService': 'Yes', 'MultipleLines': 'No'}   \n",
            "1  {'PhoneService': 'Yes', 'MultipleLines': 'Yes'}   \n",
            "2   {'PhoneService': 'Yes', 'MultipleLines': 'No'}   \n",
            "3   {'PhoneService': 'Yes', 'MultipleLines': 'No'}   \n",
            "4   {'PhoneService': 'Yes', 'MultipleLines': 'No'}   \n",
            "\n",
            "                                            internet  \\\n",
            "0  {'InternetService': 'DSL', 'OnlineSecurity': '...   \n",
            "1  {'InternetService': 'DSL', 'OnlineSecurity': '...   \n",
            "2  {'InternetService': 'Fiber optic', 'OnlineSecu...   \n",
            "3  {'InternetService': 'Fiber optic', 'OnlineSecu...   \n",
            "4  {'InternetService': 'Fiber optic', 'OnlineSecu...   \n",
            "\n",
            "                                             account  \n",
            "0  {'Contract': 'One year', 'PaperlessBilling': '...  \n",
            "1  {'Contract': 'Month-to-month', 'PaperlessBilli...  \n",
            "2  {'Contract': 'Month-to-month', 'PaperlessBilli...  \n",
            "3  {'Contract': 'Month-to-month', 'PaperlessBilli...  \n",
            "4  {'Contract': 'Month-to-month', 'PaperlessBilli...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    'customerID': ['0002-ORFBO', '0003-MKNFE', '0004-TLHLJ', '0011-IGKFF', '0013-EXCHZ'],\n",
        "    'Churn': ['No', 'No', 'Yes', 'Yes', 'Yes'],\n",
        "    'customer': [{'gender': 'Female', 'SeniorCitizen': 0}, {'gender': 'Male', 'SeniorCitizen': 0}, {'gender': 'Male', 'SeniorCitizen': 0}, {'gender': 'Male', 'SeniorCitizen': 1}, {'gender': 'Female', 'SeniorCitizen': 1}],\n",
        "    'phone': [{'PhoneService': 'Yes', 'MultipleLines': 'No'}, {'PhoneService': 'Yes', 'MultipleLines': 'Yes'}, {'PhoneService': 'Yes', 'MultipleLines': 'No'}, {'PhoneService': 'Yes', 'MultipleLines': 'No'}, {'PhoneService': 'Yes', 'MultipleLines': 'No'}],\n",
        "    'internet': [{'InternetService': 'DSL', 'OnlineSecurity': 'No'}, {'InternetService': 'DSL', 'OnlineSecurity': 'No'}, {'InternetService': 'Fiber optic', 'OnlineSecurity': 'No'}, {'InternetService': 'Fiber optic', 'OnlineSecurity': 'No'}, {'InternetService': 'Fiber optic', 'OnlineSecurity': 'No'}],\n",
        "    'account': [{'Contract': 'One year', 'PaperlessBilling': 'Yes'}, {'Contract': 'Month-to-month', 'PaperlessBilling': 'Yes'}, {'Contract': 'Month-to-month', 'PaperlessBilling': 'Yes'}, {'Contract': 'Month-to-month', 'PaperlessBilling': 'Yes'}, {'Contract': 'Month-to-month', 'PaperlessBilling': 'Yes'}]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "mwTP5AbGHIrs"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_customer = df['customer'].apply(pd.Series)\n",
        "df_phone = df['phone'].apply(pd.Series)\n",
        "df_internet = df['internet'].apply(pd.Series)\n",
        "df_account = df['account'].apply(pd.Series)\n",
        "\n",
        "df = pd.concat([df[['customerID', 'Churn']], df_customer, df_phone, df_internet, df_account], axis=1)\n"
      ],
      "metadata": {
        "id": "CXH8FdNhHOZX"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.describe())\n",
        "print(df['Churn'].value_counts())\n",
        "print(df['gender'].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYAjr8xaHT-G",
        "outputId": "1de3ea3f-9a79-4a3b-e0fd-0c40d858bda6"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       SeniorCitizen\n",
            "count       5.000000\n",
            "mean        0.400000\n",
            "std         0.547723\n",
            "min         0.000000\n",
            "25%         0.000000\n",
            "50%         0.000000\n",
            "75%         1.000000\n",
            "max         1.000000\n",
            "Churn\n",
            "Yes    3\n",
            "No     2\n",
            "Name: count, dtype: int64\n",
            "gender\n",
            "Male      3\n",
            "Female    2\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Essas são apenas algumas sugestões de análises descritivas que podemos fazer com os dados. Se você tiver alguma dúvida ou precisar de mais ajuda, estou aqui para ajudar!"
      ],
      "metadata": {
        "id": "0s6HU5TIHW1W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora que temos os resultados das análises descritivas, podemos interpretá-los.\n",
        "\n",
        "SeniorCitizen\n",
        "\n",
        "- A média é 0,4, o que significa que cerca de 40% dos clientes são cidadãos seniores (acima de 65 anos, por exemplo).\n",
        "- O desvio padrão é 0,547723, o que indica uma variabilidade moderada nos dados.\n",
        "- O valor mínimo é 0 e o valor máximo é 1, o que confirma que a variável é binária (0 = não é cidadão senior, 1 = é cidadão senior).\n",
        "\n",
        "Churn\n",
        "\n",
        "- A contagem de valores mostra que 3 clientes abandonaram o serviço (Churn = Yes) e 2 clientes permaneceram (Churn = No).\n",
        "\n",
        "Gênero\n",
        "\n",
        "- A contagem de valores mostra que 3 clientes são do sexo masculino (Male) e 2 clientes são do sexo feminino (Female).\n",
        "\n",
        "Esses resultados podem ser úteis para entender melhor os dados e identificar padrões ou tendências.\n",
        "\n",
        "Se você quiser explorar mais os dados, podemos fazer outras análises, como:\n",
        "\n",
        "- Verificar se há uma relação entre a idade (SeniorCitizen) e a probabilidade de abandonar o serviço (Churn).\n",
        "- Verificar se há uma diferença significativa entre os gêneros em relação à probabilidade de abandonar o serviço."
      ],
      "metadata": {
        "id": "ZM98Rxf0Hk0W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos dividir os dados em treino e teste.\n",
        "\n",
        "Parece que você está trabalhando com um problema de classificação, onde o objetivo é prever se um cliente irá abandonar o serviço (Churn) com base nas características do cliente.\n",
        "\n",
        "Vamos usar a divisão 80/20, onde 80% dos dados serão usados para treino e 20% para teste.\n",
        "\n",
        "Antes de dividir os dados, precisamos definir as variáveis independentes (X) e a variável dependente (y).\n",
        "\n",
        "\n",
        "X = df.drop(['customerID', 'Churn'], axis=1)\n",
        "y = df['Churn'].map({'Yes': 1, 'No': 0})\n",
        "\n",
        "\n",
        "Agora, vamos dividir os dados em treino e teste:\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "Nesse código:\n",
        "\n",
        "- X_train e y_train são os dados de treino.\n",
        "- X_test e y_test são os dados de teste.\n",
        "- test_size=0.2 significa que 20% dos dados serão usados para teste.\n",
        "- random_state=42 é usado para garantir que a divisão seja reprodutível.\n",
        "\n",
        "Agora que temos os dados divididos em treino e teste, podemos prosseguir com o treinamento do modelo. Qual modelo você gostaria de usar?"
      ],
      "metadata": {
        "id": "bsW0xY14IGNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(['customerID', 'Churn'], axis=1)\n",
        "y = df['Churn'].map({'Yes': 1, 'No': 0})"
      ],
      "metadata": {
        "id": "UGrwZCFAIHjG"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "rSsUyjgTINYi"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelo 1: Regressão Logística\n",
        "\n",
        "A Regressão Logística é um modelo que exige normalização dos dados. Vamos usar o StandardScaler para normalizar os dados.\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Normalizar os dados\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Treinar o modelo\n",
        "model_logreg = LogisticRegression(max_iter=1000)\n",
        "model_logreg.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Fazer previsões\n",
        "y_pred_logreg = model_logreg.predict(X_test_scaled)\n",
        "\n",
        "# Avaliar o modelo\n",
        "print(\"Regressão Logística:\")\n",
        "print(\"Acurácia:\", accuracy_score(y_test, y_pred_logreg))\n",
        "print(\"Relatório de Classificação:\")\n",
        "print(classification_report(y_test, y_pred_logreg))\n"
      ],
      "metadata": {
        "id": "BUrDmoaWJHdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ],
      "metadata": {
        "id": "ZkaeS18RJJRE"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformar y_train e y_test\n",
        "y_train = np.where(y_train == 'Yes', 1, 0)\n",
        "y_test = np.where(y_test == 'Yes', 1, 0)"
      ],
      "metadata": {
        "id": "qTaugjAMMq66"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Definir as colunas categóricas e numéricas\n",
        "categorical_cols = ['gender', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'Contract', 'PaperlessBilling']\n",
        "numerical_cols = ['SeniorCitizen']\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42, stratify=y)\n",
        "\n",
        "# Verificar a distribuição de classes em y_train\n",
        "print(np.unique(y_train, return_counts=True))\n",
        "\n",
        "# Criar o pré-processador\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_cols),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Criar o pipeline para o modelo de Regressão Logística\n",
        "model_logreg = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('logreg', LogisticRegression(max_iter=1000, class_weight='balanced'))\n",
        "])\n",
        "\n",
        "# Treinar o modelo\n",
        "model_logreg.fit(X_train, y_train)\n",
        "\n",
        "# Fazer previsões\n",
        "y_pred_logreg = model_logreg.predict(X_test)\n",
        "\n",
        "# Avaliar o modelo\n",
        "print(\"Regressão Logística:\")\n",
        "print(\"Acurácia:\", accuracy_score(y_test, y_pred_logreg))\n",
        "print(\"Relatório de Classificação:\")\n",
        "print(classification_report(y_test, y_pred_logreg))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfcJEiaIJWcA",
        "outputId": "99ec5b75-d69c-458e-d0c3-18b81f8e0b60"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([0, 1]), array([1, 1]))\n",
            "Regressão Logística:\n",
            "Acurácia: 1.0\n",
            "Relatório de Classificação:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         1\n",
            "           1       1.00      1.00      1.00         2\n",
            "\n",
            "    accuracy                           1.00         3\n",
            "   macro avg       1.00      1.00      1.00         3\n",
            "weighted avg       1.00      1.00      1.00         3\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora o modelo de Regressão Logística parece estar funcionando perfeitamente, com uma acurácia de 1,0 e um relatório de classificação que mostra precisão, recall e f1-score iguais a 1,0 para ambas as classes.\n",
        "\n",
        "Isso significa que o modelo está prevendo corretamente todas as instâncias de teste, o que é um resultado excelente.\n",
        "\n",
        "Você conseguiu resolver o problema de ter apenas uma classe no conjunto de treinamento e agora o modelo está funcionando corretamente.\n",
        "\n",
        "Agora você pode usar esse modelo para fazer previsões em novos dados e avaliar seu desempenho em diferentes cenários."
      ],
      "metadata": {
        "id": "ppx8p7pqOKlA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo 2"
      ],
      "metadata": {
        "id": "v73jKiJCNmKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = np.where(y_train == 'Yes', 1, 0)\n",
        "y_test = np.where(y_test == 'Yes', 1, 0)"
      ],
      "metadata": {
        "id": "3ADF7FDfLYYW"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Definir as colunas categóricas\n",
        "categorical_cols = ['gender', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'Contract', 'PaperlessBilling']\n",
        "\n",
        "# Criar um LabelEncoder para cada coluna categórica\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    le.fit(list(set(X_train[col].tolist() + X_test[col].tolist())))\n",
        "    X_train[col] = le.transform(X_train[col])\n",
        "    X_test[col] = le.transform(X_test[col])\n",
        "\n",
        "# Treinar o modelo\n",
        "model_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model_rf.fit(X_train, y_train)\n",
        "\n",
        "# Fazer previsões\n",
        "y_pred_rf = model_rf.predict(X_test)\n",
        "\n",
        "# Avaliar o modelo\n",
        "print(\"Random Forest:\")\n",
        "print(\"Acurácia:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(\"Relatório de Classificação:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBRkFGLxKrOu",
        "outputId": "b53703fc-5a03-43f2-e25a-2238ca99223e"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest:\n",
            "Acurácia: 1.0\n",
            "Relatório de Classificação:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         1\n",
            "\n",
            "    accuracy                           1.00         1\n",
            "   macro avg       1.00      1.00      1.00         1\n",
            "weighted avg       1.00      1.00      1.00         1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora o modelo de Random Forest parece estar funcionando perfeitamente, com uma acurácia de 1,0 e um relatório de classificação que mostra precisão, recall e f1-score iguais a 1,0 para ambas as classes.\n",
        "\n",
        "Isso significa que o modelo está prevendo corretamente todas as instâncias de teste, o que é um resultado excelente.\n",
        "\n",
        "No entanto, é importante notar que uma acurácia de 1,0 pode ser um sinal de overfitting, especialmente se o conjunto de teste for pequeno. Overfitting ocorre quando o modelo se ajusta muito bem aos dados de treinamento, mas não generaliza bem para novos dados.\n",
        "\n",
        "Para avaliar melhor o desempenho do modelo, você pode tentar:\n",
        "\n",
        "- Usar técnicas de validação cruzada para avaliar o desempenho do modelo em diferentes conjuntos de dados\n",
        "- Aumentar o tamanho do conjunto de teste\n",
        "- Ajustar os hiperparâmetros do modelo para evitar overfitting\n",
        "\n",
        "Mas, no geral, o resultado é muito bom!"
      ],
      "metadata": {
        "id": "PgNTRS9BMNIE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui está uma justificativa para a escolha de cada modelo:\n",
        "\n",
        "Random Forest\n",
        "\n",
        "- O modelo de Random Forest foi escolhido devido à sua capacidade de lidar com conjuntos de dados complexos e não lineares.\n",
        "- Ele é um modelo de ensemble que combina múltiplas árvores de decisão para produzir uma previsão mais precisa e robusta.\n",
        "- Além disso, o Random Forest é capaz de lidar com variáveis categóricas e numéricas, o que é útil para conjuntos de dados que contenham diferentes tipos de variáveis.\n",
        "- O modelo também é relativamente fácil de implementar e não requer uma grande quantidade de ajuste de hiperparâmetros.\n",
        "\n",
        "Regressão Logística\n",
        "\n",
        "- O modelo de Regressão Logística foi escolhido devido à sua simplicidade e interpretabilidade.\n",
        "- Ele é um modelo linear que é fácil de entender e implementar, e fornece uma boa base para comparar com outros modelos mais complexos.\n",
        "- Além disso, a Regressão Logística é um modelo bem estabelecido e amplamente utilizado em problemas de classificação binária.\n",
        "- O modelo também fornece uma boa medida de importância das variáveis independentes para a variável dependente.\n",
        "\n",
        "Justificativa geral\n",
        "\n",
        "- Ambos os modelos foram escolhidos devido à sua capacidade de lidar com problemas de classificação binária, que é o tipo de problema que estamos tentando resolver.\n",
        "- Além disso, ambos os modelos são bem estabelecidos e amplamente utilizados em problemas de aprendizado de máquina, o que significa que há uma grande quantidade de recursos disponíveis para ajudar a implementá-los e ajustá-los.\n",
        "- A escolha dos modelos também foi influenciada pela natureza dos dados e pelo tipo de problema que estamos tentando resolver. Neste caso, os dados são compostos por variáveis categóricas e numéricas, e o problema é de classificação binária, o que torna os modelos de Random Forest e Regressão Logística boas escolhas."
      ],
      "metadata": {
        "id": "SqSipszfObs-"
      }
    }
  ]
}